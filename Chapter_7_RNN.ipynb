{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 7 RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dYiLAvlzwwvg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "TEXT = torchtext.legacy.data.Field(lower=True, fix_length=200, batch_first=False)\n",
        "LABEL = torchtext.legacy.data.Field(sequential=False)"
      ],
      "metadata": {
        "id": "bqDWzjd7z67V"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy import datasets\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "metadata": {
        "id": "IVuYlFB20zuA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OiWWK6n07sy",
        "outputId": "4604b070-2cc9-44d5-f960-93ac12ee79c2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ['when', 'i', 'first', 'went', 'to', 'watch', 'the', 'shining', 'i', 'was', 'expecting', 'a', 'decent', 'film', 'from', 'what', 'i', 'had', 'heard', 'about', 'it', 'and', 'i', 'liked', 'a', 'lot', 'of', 'stanley', \"kubrick's\", 'other', 'work', 'but', 'when', 'i', 'started', 'to', 'watch', 'it', 'it', 'was', 'so', 'much', 'better', 'than', 'i', 'thought', 'it', 'would', 'be.at', 'times', 'i', 'seriously', 'felt', 'ridiculously', 'uneasy', 'and', 'i', \"couldn't\", 'take', 'my', 'eyes', 'of', 'the', 'screen', 'still', \"there's\", 'something', 'very', 'disturbing', 'about', 'everything', 'in', 'the', 'film.', 'now', 'some', 'people', \"don't\", 'like', \"kubrick's\", 'version', 'of', 'the', 'shining', 'since', 'it', \"doesn't\", 'entirely', 'follow', 'stephen', \"king's\", 'book', 'but', 'in', 'my', 'opinion', 'both', \"kubrick's\", 'version,the', 'mini-series', 'and', 'the', 'book', 'are', 'all', 'great.jack', 'nicholson', 'gives', 'an', 'awesome', 'performance.if', 'you', 'are', 'looking', 'for', 'a', 'good', 'original', 'movie', 'that', 'will', 'keep', 'you', 'thinking', 'even', 'after', 'the', 'movies', 'over', 'then', 'watch', 'the', 'shining.'], 'label': 'pos'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "for example in train_data.examples:\n",
        "  text = [x.lower() for x in vars(example)['text']]\n",
        "  text = [x.replace('<br', '') for x in text]\n",
        "  text = [''.join(c for c in s if c not in string.punctuation) for s in text]\n",
        "  text = [s for s in text if s]\n",
        "  vars(example)['text'] = text"
      ],
      "metadata": {
        "id": "zva8hWGk15S5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(0), split_ratio=0.8)"
      ],
      "metadata": {
        "id": "Ze1S25D822cX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj12myYe3ATD",
        "outputId": "648c86d7-a198-4e15-9f49-8537ea7d50e7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 20000\n",
            "Number of validation examples: 5000\n",
            "Number of testing examples: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train_data, max_size=10000, min_freq=10, vectors=None)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f'Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}')\n",
        "print(f'Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL-P107l3QIS",
        "outputId": "c1fecb56-e1d2-4c92-c7bb-f4b8dab5c087"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tokens in TEXT vocabulary: 10002\n",
            "Unique tokens in LABEL vocabulary: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_size = 300\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = torchtext.legacy.data.BucketIterator.splits((train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)"
      ],
      "metadata": {
        "id": "s9VFtjPF4Oeh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCell_Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_size):\n",
        "    super().__init__()\n",
        "    self.rnn = nn.RNNCell(input_dim, hidden_size)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    bz = inputs.shape[1]\n",
        "    ht = torch.zeros((bz, hidden_size)).to(device)\n",
        "    for word in inputs:\n",
        "      ht = self.rnn(word, ht)\n",
        "    return ht\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.em = nn.Embedding(len(TEXT.vocab.stoi), embedding_dim)\n",
        "    self.rnn = RNNCell_Encoder(embedding_dim, hidden_size)\n",
        "    self.fc1 = nn.Linear(hidden_size, 256)\n",
        "    self.fc2 = nn.Linear(256, 3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.em(x)\n",
        "    x = self.rnn(x)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "yJx9esh54yo2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "1IlK2vTE6O7J"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(epoch, model, trainloader, validloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  running_loss = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for b in trainloader:\n",
        "    x, y = b.text, b.label\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    y_pred = model(x)\n",
        "    loss = criterion(y_pred, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "      y_pred = torch.argmax(y_pred, dim=1)\n",
        "      correct += (y_pred == y).sum().item()\n",
        "      total += y.size(0)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "  epoch_loss = running_loss / len(trainloader.dataset)\n",
        "  epoch_acc = correct / total\n",
        "\n",
        "  valid_correct = 0\n",
        "  valid_total = 0\n",
        "  valid_running_loss = 0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for b in validloader:\n",
        "      x, y = b.text, b.label\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      y_pred = model(x)\n",
        "      loss = criterion(y_pred, y)\n",
        "      y_pred = torch.argmax(y_pred, dim=1)\n",
        "      valid_correct += (y_pred == y).sum().item()\n",
        "      valid_total += y.size(0)\n",
        "      valid_running_loss += loss.item()\n",
        "\n",
        "  epoch_valid_loss = valid_running_loss / len(validloader.dataset)\n",
        "  epoch_valid_acc = valid_correct / valid_total\n",
        "\n",
        "  print('epoch: ', epoch)\n",
        "  print('loss: ', round(epoch_loss, 3))\n",
        "  print('accuracy: ', round(epoch_acc, 3))\n",
        "  print('valid_loss: ', round(epoch_valid_loss, 3))\n",
        "  print('valid_accuracy: ', round(epoch_valid_acc, 3))\n",
        "\n",
        "  return epoch_loss, epoch_acc, epoch_valid_loss, epoch_valid_acc"
      ],
      "metadata": {
        "id": "-C_6hEtT8fQr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 40\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "valid_loss = []\n",
        "valid_acc = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  epoch_loss, epoch_acc, epoch_valid_loss, epoch_valid_acc = training(epoch, model, train_iterator, valid_iterator)\n",
        "  train_loss.append(epoch_loss)\n",
        "  train_acc.append(epoch_acc)\n",
        "  valid_loss.append(epoch_valid_loss)\n",
        "  valid_acc.append(epoch_valid_acc)\n",
        "\n",
        "end = time.time()\n",
        "print(f'Training done in {(end-start) / 60 :.0f}m {(end-start) % 60 :.0f}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy7e0N9Z_LWR",
        "outputId": "b23c75f5-fc7f-4130-8c7e-46aa02c903a0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0\n",
            "loss:  0.011\n",
            "accuracy:  0.499\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.507\n",
            "epoch:  1\n",
            "loss:  0.011\n",
            "accuracy:  0.503\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.5\n",
            "epoch:  2\n",
            "loss:  0.011\n",
            "accuracy:  0.512\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.494\n",
            "epoch:  3\n",
            "loss:  0.011\n",
            "accuracy:  0.52\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.495\n",
            "epoch:  4\n",
            "loss:  0.011\n",
            "accuracy:  0.523\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.507\n",
            "epoch:  5\n",
            "loss:  0.011\n",
            "accuracy:  0.533\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.5\n",
            "epoch:  6\n",
            "loss:  0.011\n",
            "accuracy:  0.538\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.52\n",
            "epoch:  7\n",
            "loss:  0.011\n",
            "accuracy:  0.543\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.504\n",
            "epoch:  8\n",
            "loss:  0.011\n",
            "accuracy:  0.556\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.519\n",
            "epoch:  9\n",
            "loss:  0.01\n",
            "accuracy:  0.56\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.512\n",
            "epoch:  10\n",
            "loss:  0.01\n",
            "accuracy:  0.566\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.506\n",
            "epoch:  11\n",
            "loss:  0.01\n",
            "accuracy:  0.57\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.502\n",
            "epoch:  12\n",
            "loss:  0.01\n",
            "accuracy:  0.578\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.518\n",
            "epoch:  13\n",
            "loss:  0.01\n",
            "accuracy:  0.588\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.54\n",
            "epoch:  14\n",
            "loss:  0.01\n",
            "accuracy:  0.597\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.527\n",
            "epoch:  15\n",
            "loss:  0.01\n",
            "accuracy:  0.593\n",
            "valid_loss:  0.012\n",
            "valid_accuracy:  0.515\n",
            "epoch:  16\n",
            "loss:  0.01\n",
            "accuracy:  0.611\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.556\n",
            "epoch:  17\n",
            "loss:  0.009\n",
            "accuracy:  0.609\n",
            "valid_loss:  0.012\n",
            "valid_accuracy:  0.526\n",
            "epoch:  18\n",
            "loss:  0.009\n",
            "accuracy:  0.623\n",
            "valid_loss:  0.012\n",
            "valid_accuracy:  0.538\n",
            "epoch:  19\n",
            "loss:  0.009\n",
            "accuracy:  0.614\n",
            "valid_loss:  0.012\n",
            "valid_accuracy:  0.504\n",
            "epoch:  20\n",
            "loss:  0.009\n",
            "accuracy:  0.631\n",
            "valid_loss:  0.012\n",
            "valid_accuracy:  0.527\n",
            "epoch:  21\n",
            "loss:  0.009\n",
            "accuracy:  0.639\n",
            "valid_loss:  0.012\n",
            "valid_accuracy:  0.536\n",
            "epoch:  22\n",
            "loss:  0.009\n",
            "accuracy:  0.647\n",
            "valid_loss:  0.013\n",
            "valid_accuracy:  0.524\n",
            "epoch:  23\n",
            "loss:  0.008\n",
            "accuracy:  0.658\n",
            "valid_loss:  0.013\n",
            "valid_accuracy:  0.521\n",
            "epoch:  24\n",
            "loss:  0.009\n",
            "accuracy:  0.651\n",
            "valid_loss:  0.013\n",
            "valid_accuracy:  0.534\n",
            "epoch:  25\n",
            "loss:  0.008\n",
            "accuracy:  0.667\n",
            "valid_loss:  0.014\n",
            "valid_accuracy:  0.529\n",
            "epoch:  26\n",
            "loss:  0.008\n",
            "accuracy:  0.672\n",
            "valid_loss:  0.014\n",
            "valid_accuracy:  0.533\n",
            "epoch:  27\n",
            "loss:  0.008\n",
            "accuracy:  0.674\n",
            "valid_loss:  0.014\n",
            "valid_accuracy:  0.541\n",
            "epoch:  28\n",
            "loss:  0.008\n",
            "accuracy:  0.687\n",
            "valid_loss:  0.015\n",
            "valid_accuracy:  0.522\n",
            "epoch:  29\n",
            "loss:  0.008\n",
            "accuracy:  0.686\n",
            "valid_loss:  0.016\n",
            "valid_accuracy:  0.518\n",
            "epoch:  30\n",
            "loss:  0.007\n",
            "accuracy:  0.694\n",
            "valid_loss:  0.017\n",
            "valid_accuracy:  0.538\n",
            "epoch:  31\n",
            "loss:  0.007\n",
            "accuracy:  0.763\n",
            "valid_loss:  0.016\n",
            "valid_accuracy:  0.615\n",
            "epoch:  32\n",
            "loss:  0.007\n",
            "accuracy:  0.722\n",
            "valid_loss:  0.017\n",
            "valid_accuracy:  0.647\n",
            "epoch:  33\n",
            "loss:  0.008\n",
            "accuracy:  0.691\n",
            "valid_loss:  0.017\n",
            "valid_accuracy:  0.518\n",
            "epoch:  34\n",
            "loss:  0.007\n",
            "accuracy:  0.687\n",
            "valid_loss:  0.018\n",
            "valid_accuracy:  0.521\n",
            "epoch:  35\n",
            "loss:  0.007\n",
            "accuracy:  0.696\n",
            "valid_loss:  0.019\n",
            "valid_accuracy:  0.518\n",
            "epoch:  36\n",
            "loss:  0.007\n",
            "accuracy:  0.7\n",
            "valid_loss:  0.02\n",
            "valid_accuracy:  0.526\n",
            "epoch:  37\n",
            "loss:  0.007\n",
            "accuracy:  0.703\n",
            "valid_loss:  0.021\n",
            "valid_accuracy:  0.516\n",
            "epoch:  38\n",
            "loss:  0.007\n",
            "accuracy:  0.71\n",
            "valid_loss:  0.022\n",
            "valid_accuracy:  0.515\n",
            "epoch:  39\n",
            "loss:  0.006\n",
            "accuracy:  0.714\n",
            "valid_loss:  0.024\n",
            "valid_accuracy:  0.53\n",
            "Training done in 16m 29s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(epoch, model, testloader):\n",
        "  test_correct = 0\n",
        "  test_total = 0\n",
        "  test_running_loss = 0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for b in testloader:\n",
        "      x, y = b.text, b.label\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      y_pred = model(x)\n",
        "      loss = criterion(y_pred, y)\n",
        "      y_pred = torch.argmax(y_pred, dim=1)\n",
        "      test_correct += (y_pred == y).sum().item()\n",
        "      test_total += y.size(0)\n",
        "      test_running_loss += loss.item()\n",
        "  \n",
        "  epoch_test_loss = test_running_loss / len(testloader.dataset)\n",
        "  epoch_test_acc = test_correct / test_total\n",
        "\n",
        "  print('-----------------------------------------')\n",
        "  print('epoch: ', epoch)\n",
        "  print('loss: ', round(epoch_test_loss, 3))\n",
        "  print('accuracy: ', round(epoch_test_acc, 3))\n",
        "\n",
        "  return epoch_test_loss, epoch_test_acc"
      ],
      "metadata": {
        "id": "SmpWT9ZuAAQb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15\n",
        "test_loss = []\n",
        "test_acc = []\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "  epoch_test_loss, epoch_test_acc = evaluate(epoch, model, test_iterator)\n",
        "  test_loss.append(epoch_test_loss)\n",
        "  test_acc.append(epoch_test_acc)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(f'Training done in {(end-start) / 60 :.0f}m {(end-start) % 60 :.0f}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnBIn25bhP-4",
        "outputId": "a828651d-9c78-401b-c5b4-03322cd24aa2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "epoch:  1\n",
            "loss:  0.021\n",
            "accuracy:  0.517\n",
            "-----------------------------------------\n",
            "epoch:  2\n",
            "loss:  0.021\n",
            "accuracy:  0.517\n",
            "-----------------------------------------\n",
            "epoch:  3\n",
            "loss:  0.021\n",
            "accuracy:  0.517\n",
            "-----------------------------------------\n",
            "epoch:  4\n",
            "loss:  0.021\n",
            "accuracy:  0.517\n",
            "-----------------------------------------\n",
            "epoch:  5\n",
            "loss:  0.021\n",
            "accuracy:  0.517\n",
            "-----------------------------------------\n",
            "epoch:  6\n",
            "loss:  0.021\n",
            "accuracy:  0.517\n",
            "-----------------------------------------\n",
            "epoch:  7\n",
            "loss:  0.021\n",
            "accuracy:  0.517\n",
            "-----------------------------------------\n",
            "epoch:  8\n",
            "loss:  0.021\n",
            "accuracy:  0.517\n",
            "-----------------------------------------\n",
            "epoch:  9\n",
            "loss:  0.021\n",
            "accuracy:  0.517\n",
            "-----------------------------------------\n",
            "epoch:  10\n",
            "loss:  0.021\n",
            "accuracy:  0.517\n",
            "-----------------------------------------\n",
            "epoch:  11\n",
            "loss:  0.021\n",
            "accuracy:  0.517\n",
            "-----------------------------------------\n",
            "epoch:  12\n",
            "loss:  0.021\n",
            "accuracy:  0.517\n",
            "-----------------------------------------\n",
            "epoch:  13\n",
            "loss:  0.021\n",
            "accuracy:  0.517\n",
            "-----------------------------------------\n",
            "epoch:  14\n",
            "loss:  0.021\n",
            "accuracy:  0.517\n",
            "-----------------------------------------\n",
            "epoch:  15\n",
            "loss:  0.021\n",
            "accuracy:  0.517\n",
            "Training done in 3m 37s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n"
      ],
      "metadata": {
        "id": "lDC43SIz3UOF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "TEXT = torchtext.legacy.data.Field(sequential=True, batch_first=True, lower=True)\n",
        "LABEL = torchtext.legacy.data.Field(sequential=False, batch_first=True)\n",
        "\n",
        "from torchtext.legacy import datasets\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split(split_ratio=0.8)\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size=10000, min_freq=10, vectors=None)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Csl5bmTw3iQF",
        "outputId": "fb1ac8da-a7ba-4e60-e1a9-eefa67bc14dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84.1M/84.1M [00:02<00:00, 30.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_iterator, valid_iterator, test_iterator = torchtext.legacy.data.BucketIterator.splits((train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)"
      ],
      "metadata": {
        "id": "0e4S1wjS4LX7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(TEXT.vocab)\n",
        "n_classes = 2 # Pos, Neg"
      ],
      "metadata": {
        "id": "-ca-pjf44nOO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicRNN(nn.Module):\n",
        "  def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
        "    super().__init__()\n",
        "    self.n_layers = n_layers\n",
        "    self.embed = nn.Embedding(n_vocab, embed_dim)\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "    self.rnn = nn.RNN(embed_dim, self.hidden_dim, num_layers=self.n_layers, batch_first=True)\n",
        "    self.out = nn.Linear(self.hidden_dim, n_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embed(x)\n",
        "    h_0 = self._init_state(batch_size=x.size(0))\n",
        "    x, _ = self.rnn(x, h_0)\n",
        "    h_t = x[:, -1, :]\n",
        "    self.dropout(h_t)\n",
        "    logit = torch.sigmoid(self.out(h_t))\n",
        "    return logit\n",
        "\n",
        "  def _init_state(self, batch_size=1):\n",
        "    weight = next(self.parameters()).data\n",
        "    return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()"
      ],
      "metadata": {
        "id": "KgNVd5jW4rMu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BasicRNN(n_layers=1, hidden_dim=256, n_vocab=vocab_size, embed_dim=128, n_classes=n_classes, dropout_p=0.5)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "wD-3lUQr6HhG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, train_iter):\n",
        "  model.train()\n",
        "  for b, batch in enumerate(train_iter):\n",
        "    x, y = batch.text.to(device), batch.label.to(device)\n",
        "    y.data.sub_(1)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logit = model(x)\n",
        "    loss = F.cross_entropy(logit, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if b % 50 == 0:\n",
        "      print(f'Train Epoch: {b} [{b * len(x)}/{len(train_iter.dataset)} ({(b * len(x))/(len(train_iter.dataset)) * 100 :.0f})%]\\tLoss: {loss.item():.6f}')"
      ],
      "metadata": {
        "id": "rKs9snC06mkH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_iter):\n",
        "  model.eval()\n",
        "  corrects, total, total_loss = 0, 0, 0\n",
        "\n",
        "  for batch in val_iter:\n",
        "    x, y = batch.text.to(device), batch.label.to(device)\n",
        "    y.data.sub_(1)\n",
        "    logit = model(x)\n",
        "    loss = F.cross_entropy(logit, y, reduction=\"sum\")\n",
        "    total += y.size(0)\n",
        "    total_loss += loss.item()\n",
        "    corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
        "\n",
        "  avg_loss = total_loss / len(val_iter.dataset)\n",
        "  avg_accuracy = corrects / total\n",
        "  return avg_loss, avg_accuracy"
      ],
      "metadata": {
        "id": "RyHhwdFi9DSw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100\n",
        "LR = 0.001\n",
        "EPOCHS = 10\n",
        "\n",
        "for e in range(1, EPOCHS + 1):\n",
        "  train(model, optimizer, train_iterator)\n",
        "  val_loss, val_accuracy = evaluate(model, valid_iterator)\n",
        "  print(f'[EPOCH: {e}], Validation Loss: {val_loss:.2f} | Validation Accuracy: {val_accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0heSTml49ml2",
        "outputId": "8f34d468-8c7f-445a-8f8b-ba3840ec19f2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/20000 (0)%]\tLoss: 0.686803\n",
            "Train Epoch: 50 [5000/20000 (25)%]\tLoss: 0.692514\n",
            "Train Epoch: 100 [10000/20000 (50)%]\tLoss: 0.696747\n",
            "Train Epoch: 150 [15000/20000 (75)%]\tLoss: 0.694003\n",
            "[EPOCH: 1], Validation Loss: 0.69 | Validation Accuracy: 0.51%\n",
            "Train Epoch: 0 [0/20000 (0)%]\tLoss: 0.692554\n",
            "Train Epoch: 50 [5000/20000 (25)%]\tLoss: 0.694496\n",
            "Train Epoch: 100 [10000/20000 (50)%]\tLoss: 0.691629\n",
            "Train Epoch: 150 [15000/20000 (75)%]\tLoss: 0.690907\n",
            "[EPOCH: 2], Validation Loss: 0.69 | Validation Accuracy: 0.50%\n",
            "Train Epoch: 0 [0/20000 (0)%]\tLoss: 0.693846\n",
            "Train Epoch: 50 [5000/20000 (25)%]\tLoss: 0.692319\n",
            "Train Epoch: 100 [10000/20000 (50)%]\tLoss: 0.691828\n",
            "Train Epoch: 150 [15000/20000 (75)%]\tLoss: 0.693744\n",
            "[EPOCH: 3], Validation Loss: 0.69 | Validation Accuracy: 0.50%\n",
            "Train Epoch: 0 [0/20000 (0)%]\tLoss: 0.693795\n",
            "Train Epoch: 50 [5000/20000 (25)%]\tLoss: 0.691414\n",
            "Train Epoch: 100 [10000/20000 (50)%]\tLoss: 0.692201\n",
            "Train Epoch: 150 [15000/20000 (75)%]\tLoss: 0.692041\n",
            "[EPOCH: 4], Validation Loss: 0.69 | Validation Accuracy: 0.50%\n",
            "Train Epoch: 0 [0/20000 (0)%]\tLoss: 0.694147\n",
            "Train Epoch: 50 [5000/20000 (25)%]\tLoss: 0.693387\n",
            "Train Epoch: 100 [10000/20000 (50)%]\tLoss: 0.694313\n",
            "Train Epoch: 150 [15000/20000 (75)%]\tLoss: 0.694478\n",
            "[EPOCH: 5], Validation Loss: 0.69 | Validation Accuracy: 0.50%\n",
            "Train Epoch: 0 [0/20000 (0)%]\tLoss: 0.692477\n",
            "Train Epoch: 50 [5000/20000 (25)%]\tLoss: 0.692942\n",
            "Train Epoch: 100 [10000/20000 (50)%]\tLoss: 0.693152\n",
            "Train Epoch: 150 [15000/20000 (75)%]\tLoss: 0.691424\n",
            "[EPOCH: 6], Validation Loss: 0.69 | Validation Accuracy: 0.50%\n",
            "Train Epoch: 0 [0/20000 (0)%]\tLoss: 0.691486\n",
            "Train Epoch: 50 [5000/20000 (25)%]\tLoss: 0.695104\n",
            "Train Epoch: 100 [10000/20000 (50)%]\tLoss: 0.693188\n",
            "Train Epoch: 150 [15000/20000 (75)%]\tLoss: 0.692152\n",
            "[EPOCH: 7], Validation Loss: 0.69 | Validation Accuracy: 0.50%\n",
            "Train Epoch: 0 [0/20000 (0)%]\tLoss: 0.692372\n",
            "Train Epoch: 50 [5000/20000 (25)%]\tLoss: 0.689038\n",
            "Train Epoch: 100 [10000/20000 (50)%]\tLoss: 0.689144\n",
            "Train Epoch: 150 [15000/20000 (75)%]\tLoss: 0.690480\n",
            "[EPOCH: 8], Validation Loss: 0.70 | Validation Accuracy: 0.50%\n",
            "Train Epoch: 0 [0/20000 (0)%]\tLoss: 0.693069\n",
            "Train Epoch: 50 [5000/20000 (25)%]\tLoss: 0.694236\n",
            "Train Epoch: 100 [10000/20000 (50)%]\tLoss: 0.696325\n",
            "Train Epoch: 150 [15000/20000 (75)%]\tLoss: 0.691550\n",
            "[EPOCH: 9], Validation Loss: 0.70 | Validation Accuracy: 0.50%\n",
            "Train Epoch: 0 [0/20000 (0)%]\tLoss: 0.694221\n",
            "Train Epoch: 50 [5000/20000 (25)%]\tLoss: 0.694039\n",
            "Train Epoch: 100 [10000/20000 (50)%]\tLoss: 0.690298\n",
            "Train Epoch: 150 [15000/20000 (75)%]\tLoss: 0.692860\n",
            "[EPOCH: 10], Validation Loss: 0.70 | Validation Accuracy: 0.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dalS5azA-UtP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}