{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 7 RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dYiLAvlzwwvg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "TEXT = torchtext.legacy.data.Field(lower=True, fix_length=200, batch_first=False)\n",
        "LABEL = torchtext.legacy.data.Field(sequential=False)"
      ],
      "metadata": {
        "id": "bqDWzjd7z67V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy import datasets\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVuYlFB20zuA",
        "outputId": "d878dc50-f0d7-441a-e081-21d196c0075b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84.1M/84.1M [00:02<00:00, 30.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OiWWK6n07sy",
        "outputId": "ddc7294a-0101-45a9-f5a0-7377acbbe343"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ['spoilers.', 'like', 'other', 'posters,', 'i', 'felt', 'that', 'the', 'ending', 'was', 'a', 'bit', 'abrupt.', 'i', 'would', 'have', 'liked', 'to', 'have', 'seen', 'the', 'crew', 'adjusting', 'to', 'life', 'back', 'on', 'earth', 'after', 'their', 'return.', 'i', 'suppose', 'the', 'writers', 'anticipated', 'this', 'problem', 'by', '\"front', 'loading\"', 'some', 'voyager', 'on', 'earth', 'sequences', 'at', 'the', 'beginning', 'of', 'the', 'episode.', '(of', 'course,', 'that', 'time', 'line', 'has', 'been', 'eradicated,', 'so', \"it's\", 'all', 'moot.)', 'i', 'did', 'like', 'how', 'admiral', 'janeway', 'died', 'for', 'the', 'voyager', 'crew.', 'as', 'fans,', 'we', 'get', 'to', 'have', 'our', 'cake', 'and', 'eat', 'it', 'to,', 'by', 'having', 'janeway', 'both', 'make', 'the', 'ultimate', 'sacrifice', 'and', 'live', 'on.', 'i', 'admit', 'that', 'the', 'scenes', 'of', 'janeway', 'and', 'her', 'older', 'self', 'having', 'conversations', 'was', 'bizarre', 'and', 'so', 'easily', 'could', 'have', 'crossed', 'the', 'line', 'into', 'camp.', 'fortunately,', 'mulgrew(s)', 'pulled', 'it', 'off.'], 'label': 'pos'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "for example in train_data.examples:\n",
        "  text = [x.lower() for x in vars(example)['text']]\n",
        "  text = [x.replace('<br', '') for x in text]\n",
        "  text = [''.join(c for c in s if c not in string.punctuation) for s in text]\n",
        "  text = [s for s in text if s]\n",
        "  vars(example)['text'] = text"
      ],
      "metadata": {
        "id": "zva8hWGk15S5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(0), split_ratio=0.8)"
      ],
      "metadata": {
        "id": "Ze1S25D822cX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj12myYe3ATD",
        "outputId": "cb3d63c1-efcc-419a-a1a6-c0bb1109b777"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 20000\n",
            "Number of validation examples: 5000\n",
            "Number of testing examples: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train_data, max_size=10000, min_freq=10, vectors=None)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f'Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}')\n",
        "print(f'Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL-P107l3QIS",
        "outputId": "7eba91b2-60e3-4f58-9859-4c5ee12bde14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tokens in TEXT vocabulary: 10002\n",
            "Unique tokens in LABEL vocabulary: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_size = 300\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = torchtext.legacy.data.BucketIterator.splits((train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)"
      ],
      "metadata": {
        "id": "s9VFtjPF4Oeh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCell_Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_size):\n",
        "    super().__init__()\n",
        "    self.rnn = nn.RNNCell(input_dim, hidden_size)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    bz = inputs.shape[1]\n",
        "    ht = torch.zeros((bz, hidden_size)).to(device)\n",
        "    for word in inputs:\n",
        "      ht = self.rnn(word, ht)\n",
        "    return ht\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.em = nn.Embedding(len(TEXT.vocab.stoi), embedding_dim)\n",
        "    self.rnn = RNNCell_Encoder(embedding_dim, hidden_size)\n",
        "    self.fc1 = nn.Linear(hidden_size, 256)\n",
        "    self.fc2 = nn.Linear(256, 3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.em(x)\n",
        "    x = self.rnn(x)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "yJx9esh54yo2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "1IlK2vTE6O7J"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(epoch, model, trainloader, validloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  running_loss = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for b in trainloader:\n",
        "    x, y = b.text, b.label\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    y_pred = model(x)\n",
        "    loss = criterion(y_pred, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "      y_pred = torch.argmax(y_pred, dim=1)\n",
        "      correct += (y_pred == y).sum().item()\n",
        "      total += y.size(0)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "  epoch_loss = running_loss / len(trainloader.dataset)\n",
        "  epoch_acc = correct / total\n",
        "\n",
        "  valid_correct = 0\n",
        "  valid_total = 0\n",
        "  valid_running_loss = 0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for b in validloader:\n",
        "      x, y = b.text, b.label\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      y_pred = model(x)\n",
        "      loss = criterion(y_pred, y)\n",
        "      y_pred = torch.argmax(y_pred, dim=1)\n",
        "      valid_correct += (y_pred == y).sum().item()\n",
        "      valid_total += y.size(0)\n",
        "      valid_running_loss += loss.item()\n",
        "\n",
        "  epoch_valid_loss = valid_running_loss / len(validloader.dataset)\n",
        "  epoch_valid_acc = valid_correct / valid_total\n",
        "\n",
        "  print('epoch: ', epoch)\n",
        "  print('loss: ', round(epoch_loss, 3))\n",
        "  print('accuracy: ', round(epoch_acc, 3))\n",
        "  print('valid_loss: ', round(epoch_valid_loss, 3))\n",
        "  print('valid_accuracy: ', round(epoch_valid_acc, 3))\n",
        "\n",
        "  return epoch_loss, epoch_acc, epoch_valid_loss, epoch_valid_acc"
      ],
      "metadata": {
        "id": "-C_6hEtT8fQr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "valid_loss = []\n",
        "valid_acc = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  epoch_loss, epoch_acc, epoch_valid_loss, epoch_valid_acc = training(epoch, model, train_iterator, valid_iterator)\n",
        "  train_loss.append(epoch_loss)\n",
        "  train_acc.append(epoch_acc)\n",
        "  valid_loss.append(epoch_valid_loss)\n",
        "  valid_acc.append(epoch_valid_acc)\n",
        "\n",
        "end = time.time()\n",
        "print(f'Training done in {(end-start / 60)}m {(end-start) % 60}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy7e0N9Z_LWR",
        "outputId": "40b3c364-b1ef-4605-ab60-b9614ec1ac61"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0\n",
            "loss:  0.011\n",
            "accuracy:  0.495\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.506\n",
            "epoch:  1\n",
            "loss:  0.011\n",
            "accuracy:  0.506\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.491\n",
            "epoch:  2\n",
            "loss:  0.011\n",
            "accuracy:  0.514\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.493\n",
            "epoch:  3\n",
            "loss:  0.011\n",
            "accuracy:  0.517\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.49\n",
            "epoch:  4\n",
            "loss:  0.011\n",
            "accuracy:  0.527\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.504\n",
            "epoch:  5\n",
            "loss:  0.011\n",
            "accuracy:  0.531\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.491\n",
            "epoch:  6\n",
            "loss:  0.011\n",
            "accuracy:  0.543\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.51\n",
            "epoch:  7\n",
            "loss:  0.011\n",
            "accuracy:  0.548\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.498\n",
            "epoch:  8\n",
            "loss:  0.01\n",
            "accuracy:  0.558\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.513\n",
            "epoch:  9\n",
            "loss:  0.01\n",
            "accuracy:  0.563\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.523\n",
            "epoch:  10\n",
            "loss:  0.01\n",
            "accuracy:  0.575\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.504\n",
            "epoch:  11\n",
            "loss:  0.01\n",
            "accuracy:  0.584\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.51\n",
            "epoch:  12\n",
            "loss:  0.01\n",
            "accuracy:  0.594\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.546\n",
            "epoch:  13\n",
            "loss:  0.01\n",
            "accuracy:  0.601\n",
            "valid_loss:  0.011\n",
            "valid_accuracy:  0.522\n",
            "epoch:  14\n",
            "loss:  0.01\n",
            "accuracy:  0.611\n",
            "valid_loss:  0.012\n",
            "valid_accuracy:  0.512\n",
            "Training done in 1620064345.5911384m 10.878168106079102s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SmpWT9ZuAAQb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}