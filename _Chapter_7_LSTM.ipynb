{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "\bChapter 7 LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PHM-Nqhre35m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dataset\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Parameter\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "torch.manual_seed(125)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(125)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "mnist_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, ), (1.0, ))])"
      ],
      "metadata": {
        "id": "FSax3Kczj3yt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import MNIST\n",
        "\n",
        "download_root = \"/content/drive/MyDrive/딥러닝 파이토치 교과서/\"\n",
        "\n",
        "train_dataset = MNIST(download_root, transform=mnist_transform, train=True, download=True)\n",
        "\n",
        "valid_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)\n",
        "\n",
        "test_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)"
      ],
      "metadata": {
        "id": "FCP6h708knCu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "01gFTMc-ldPG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "n_iters = 6000\n",
        "\n",
        "num_epochs = int(n_iters / (len(train_dataset) / batch_size))"
      ],
      "metadata": {
        "id": "_Zya1GONmGQa"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMCell(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, bias=True):\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.bias = bias\n",
        "    self.x2h = nn.Linear(input_size, 4 * hidden_size, bias=bias)\n",
        "    self.h2h = nn.Linear(hidden_size, 4 * hidden_size, bias=bias)\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    std = 1.0 / math.sqrt(self.hidden_size)\n",
        "    for w in self.parameters():\n",
        "      w.data.uniform_(-std, std)\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    hx, cx = hidden\n",
        "    x = x.view(-1, x.size(1))\n",
        "\n",
        "    gates = self.x2h(x) + self.h2h(hx)\n",
        "    gates = gates.squeeze()\n",
        "    ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
        "\n",
        "    ingate = F.sigmoid(ingate)\n",
        "    forgetgate = F.sigmoid(forgetgate)\n",
        "    cellgate = F.tanh(cellgate)\n",
        "    outgate = F.sigmoid(outgate)\n",
        "\n",
        "    cy = torch.mul(cx, forgetgate) + torch.mul(ingate, cellgate)\n",
        "    hy = torch.mul(outgate, F.tanh(cy))\n",
        "\n",
        "    return (hy, cy)"
      ],
      "metadata": {
        "id": "N8yckrhcmUqi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, bias=True):\n",
        "    super().__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.layer_dim = layer_dim\n",
        "    self.lstm = LSTMCell(input_dim, hidden_dim, layer_dim)\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    if torch.cuda.is_available():\n",
        "      h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda())\n",
        "    else:\n",
        "      h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      c0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda())\n",
        "    else:\n",
        "      c0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
        "\n",
        "    outs = []\n",
        "    cn = c0[0, :, :]\n",
        "    hn = h0[0, :, :]\n",
        "\n",
        "    for seq in range(x.size(1)):\n",
        "      hn, cn = self.lstm(x[:, seq, :], (hn, cn))\n",
        "      outs.append(hn)\n",
        "\n",
        "    out = outs[-1].squeeze()\n",
        "    out = self.fc(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "b7pMEBC7nr2w"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 28\n",
        "hidden_dim = 128\n",
        "layer_dim = 1\n",
        "output_dim = 10\n",
        "\n",
        "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "2nouFluhqUMg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_dim = 28\n",
        "loss_list = []\n",
        "iter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    images = Variable(images.view(-1, seq_dim, input_dim).cuda())\n",
        "    labels = Variable(labels.cuda())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels).cuda()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_list.append(loss.item())\n",
        "    iter += 1\n",
        "\n",
        "    if iter % 100 == 0:\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for images, labels in valid_loader:\n",
        "        images = Variable(images.view(-1, seq_dim, input_dim).cuda())\n",
        "        labels = Variable(labels.cuda())\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted.cpu() == labels.cpu()).sum()\n",
        "\n",
        "      accuracy = 100 * correct / total\n",
        "\n",
        "      print(f'Iteration: {iter} Loss: {loss.item()} Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z5N3Xnxq1J8",
        "outputId": "9107ce5a-6bbc-47bf-ccea-89b83fb2852c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 100 Loss: 2.293569803237915 Accuracy: 11.350000381469727\n",
            "Iteration: 200 Loss: 2.3025078773498535 Accuracy: 11.550000190734863\n",
            "Iteration: 300 Loss: 2.293407917022705 Accuracy: 15.260000228881836\n",
            "Iteration: 400 Loss: 2.299330711364746 Accuracy: 15.5600004196167\n",
            "Iteration: 500 Loss: 2.237457513809204 Accuracy: 21.420000076293945\n",
            "Iteration: 600 Loss: 1.948909044265747 Accuracy: 33.18000030517578\n",
            "Iteration: 700 Loss: 1.719103217124939 Accuracy: 47.04999923706055\n",
            "Iteration: 800 Loss: 1.2479084730148315 Accuracy: 58.810001373291016\n",
            "Iteration: 900 Loss: 0.915291428565979 Accuracy: 61.63999938964844\n",
            "Iteration: 1000 Loss: 0.7579075694084167 Accuracy: 70.6500015258789\n",
            "Iteration: 1100 Loss: 0.5575475692749023 Accuracy: 81.08999633789062\n",
            "Iteration: 1200 Loss: 0.6163272261619568 Accuracy: 83.12999725341797\n",
            "Iteration: 1300 Loss: 0.387583464384079 Accuracy: 84.80999755859375\n",
            "Iteration: 1400 Loss: 0.37117958068847656 Accuracy: 88.19000244140625\n",
            "Iteration: 1500 Loss: 0.44921135902404785 Accuracy: 89.81999969482422\n",
            "Iteration: 1600 Loss: 0.5061753988265991 Accuracy: 89.88999938964844\n",
            "Iteration: 1700 Loss: 0.2697964608669281 Accuracy: 91.23999786376953\n",
            "Iteration: 1800 Loss: 0.18996676802635193 Accuracy: 92.4000015258789\n",
            "Iteration: 1900 Loss: 0.26339101791381836 Accuracy: 93.12999725341797\n",
            "Iteration: 2000 Loss: 0.38869136571884155 Accuracy: 92.70999908447266\n",
            "Iteration: 2100 Loss: 0.20319153368473053 Accuracy: 94.52999877929688\n",
            "Iteration: 2200 Loss: 0.19840547442436218 Accuracy: 93.91999816894531\n",
            "Iteration: 2300 Loss: 0.10275374352931976 Accuracy: 95.2699966430664\n",
            "Iteration: 2400 Loss: 0.20517928898334503 Accuracy: 93.05999755859375\n",
            "Iteration: 2500 Loss: 0.14987295866012573 Accuracy: 95.25\n",
            "Iteration: 2600 Loss: 0.20428384840488434 Accuracy: 94.81999969482422\n",
            "Iteration: 2700 Loss: 0.1451529860496521 Accuracy: 95.4000015258789\n",
            "Iteration: 2800 Loss: 0.16569380462169647 Accuracy: 94.98999786376953\n",
            "Iteration: 2900 Loss: 0.07576797157526016 Accuracy: 96.37999725341797\n",
            "Iteration: 3000 Loss: 0.0828203484416008 Accuracy: 95.87000274658203\n",
            "Iteration: 3100 Loss: 0.1471015214920044 Accuracy: 96.44999694824219\n",
            "Iteration: 3200 Loss: 0.24601170420646667 Accuracy: 96.2300033569336\n",
            "Iteration: 3300 Loss: 0.10359632968902588 Accuracy: 96.25\n",
            "Iteration: 3400 Loss: 0.20945873856544495 Accuracy: 95.94000244140625\n",
            "Iteration: 3500 Loss: 0.15097592771053314 Accuracy: 96.80999755859375\n",
            "Iteration: 3600 Loss: 0.052358727902173996 Accuracy: 96.94999694824219\n",
            "Iteration: 3700 Loss: 0.09693603962659836 Accuracy: 97.02999877929688\n",
            "Iteration: 3800 Loss: 0.06562554091215134 Accuracy: 96.31999969482422\n",
            "Iteration: 3900 Loss: 0.0873606950044632 Accuracy: 96.19999694824219\n",
            "Iteration: 4000 Loss: 0.03379138559103012 Accuracy: 97.01000213623047\n",
            "Iteration: 4100 Loss: 0.08403880894184113 Accuracy: 96.7300033569336\n",
            "Iteration: 4200 Loss: 0.0156391728669405 Accuracy: 97.08999633789062\n",
            "Iteration: 4300 Loss: 0.13715633749961853 Accuracy: 97.06999969482422\n",
            "Iteration: 4400 Loss: 0.3594423830509186 Accuracy: 97.20999908447266\n",
            "Iteration: 4500 Loss: 0.029660074040293694 Accuracy: 96.8499984741211\n",
            "Iteration: 4600 Loss: 0.12132517248392105 Accuracy: 97.0199966430664\n",
            "Iteration: 4700 Loss: 0.06425412744283676 Accuracy: 97.12999725341797\n",
            "Iteration: 4800 Loss: 0.01643538475036621 Accuracy: 97.30000305175781\n",
            "Iteration: 4900 Loss: 0.07844678312540054 Accuracy: 97.4800033569336\n",
            "Iteration: 5000 Loss: 0.029410328716039658 Accuracy: 97.37999725341797\n",
            "Iteration: 5100 Loss: 0.10691625624895096 Accuracy: 97.54000091552734\n",
            "Iteration: 5200 Loss: 0.03681512549519539 Accuracy: 97.23999786376953\n",
            "Iteration: 5300 Loss: 0.07413693517446518 Accuracy: 97.55999755859375\n",
            "Iteration: 5400 Loss: 0.011522551067173481 Accuracy: 97.37000274658203\n",
            "Iteration: 5500 Loss: 0.2328355759382248 Accuracy: 97.69000244140625\n",
            "Iteration: 5600 Loss: 0.15814009308815002 Accuracy: 96.80000305175781\n",
            "Iteration: 5700 Loss: 0.11507465690374374 Accuracy: 97.66999816894531\n",
            "Iteration: 5800 Loss: 0.09741834551095963 Accuracy: 97.1500015258789\n",
            "Iteration: 5900 Loss: 0.06522783637046814 Accuracy: 97.62999725341797\n",
            "Iteration: 6000 Loss: 0.010336375795304775 Accuracy: 97.54000091552734\n",
            "Iteration: 6100 Loss: 0.18008717894554138 Accuracy: 97.41999816894531\n",
            "Iteration: 6200 Loss: 0.01911281794309616 Accuracy: 97.69000244140625\n",
            "Iteration: 6300 Loss: 0.037945862859487534 Accuracy: 97.72000122070312\n",
            "Iteration: 6400 Loss: 0.13622938096523285 Accuracy: 97.22000122070312\n",
            "Iteration: 6500 Loss: 0.1483282446861267 Accuracy: 97.61000061035156\n",
            "Iteration: 6600 Loss: 0.04896062985062599 Accuracy: 97.81999969482422\n",
            "Iteration: 6700 Loss: 0.047353558242321014 Accuracy: 97.7300033569336\n",
            "Iteration: 6800 Loss: 0.027122370898723602 Accuracy: 97.9800033569336\n",
            "Iteration: 6900 Loss: 0.14323800802230835 Accuracy: 98.04000091552734\n",
            "Iteration: 7000 Loss: 0.097136490046978 Accuracy: 97.75\n",
            "Iteration: 7100 Loss: 0.0371621698141098 Accuracy: 97.73999786376953\n",
            "Iteration: 7200 Loss: 0.055063482373952866 Accuracy: 97.7300033569336\n",
            "Iteration: 7300 Loss: 0.09858575463294983 Accuracy: 97.76000213623047\n",
            "Iteration: 7400 Loss: 0.10428984463214874 Accuracy: 98.0999984741211\n",
            "Iteration: 7500 Loss: 0.020125901326537132 Accuracy: 98.0199966430664\n",
            "Iteration: 7600 Loss: 0.0195169597864151 Accuracy: 97.87000274658203\n",
            "Iteration: 7700 Loss: 0.0348857045173645 Accuracy: 98.0\n",
            "Iteration: 7800 Loss: 0.008992678485810757 Accuracy: 97.93000030517578\n",
            "Iteration: 7900 Loss: 0.14796987175941467 Accuracy: 98.19999694824219\n",
            "Iteration: 8000 Loss: 0.020962875336408615 Accuracy: 98.19999694824219\n",
            "Iteration: 8100 Loss: 0.17120331525802612 Accuracy: 97.76000213623047\n",
            "Iteration: 8200 Loss: 0.07502871751785278 Accuracy: 98.16000366210938\n",
            "Iteration: 8300 Loss: 0.05646590515971184 Accuracy: 98.12000274658203\n",
            "Iteration: 8400 Loss: 0.048159945756196976 Accuracy: 97.87000274658203\n",
            "Iteration: 8500 Loss: 0.0664467066526413 Accuracy: 98.1500015258789\n",
            "Iteration: 8600 Loss: 0.02901843935251236 Accuracy: 97.88999938964844\n",
            "Iteration: 8700 Loss: 0.02421872317790985 Accuracy: 98.12999725341797\n",
            "Iteration: 8800 Loss: 0.03271567448973656 Accuracy: 97.98999786376953\n",
            "Iteration: 8900 Loss: 0.0643189400434494 Accuracy: 97.81999969482422\n",
            "Iteration: 9000 Loss: 0.1009548157453537 Accuracy: 97.91000366210938\n",
            "Iteration: 9100 Loss: 0.051054202020168304 Accuracy: 98.30999755859375\n",
            "Iteration: 9200 Loss: 0.07568521052598953 Accuracy: 98.08000183105469\n",
            "Iteration: 9300 Loss: 0.0585324689745903 Accuracy: 98.23999786376953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_iter):\n",
        "  corrects, total, total_loss = 0, 0, 0\n",
        "  model.eval()\n",
        "  for images, labels in val_iter:\n",
        "    images = images.view(-1, seq_dim, input_dim).cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    logit = model(images).cuda()\n",
        "    loss = F.cross_entropy(logit, labels, reduction='sum')\n",
        "    _, predicted = torch.max(logit.data, 1)\n",
        "    total += labels.size(0)\n",
        "    total_loss += loss.item()\n",
        "    corrects += (predicted == labels).sum()\n",
        "\n",
        "  avg_loss = total_loss / len(val_iter.dataset)\n",
        "  avg_accuracy = corrects / total\n",
        "  return avg_loss, avg_accuracy\n"
      ],
      "metadata": {
        "id": "OPa8kUkJsRed"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate(model, test_loader)\n",
        "print(f'Test Loss : {test_loss:.2f} | Test Accuracy: {test_acc:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSPaEb6owDOa",
        "outputId": "7d947b7f-1c88-4201-f674-8f7d37a51b22"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss : 0.07 | Test Accuracy: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oDBrZtSqyXg9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}